{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_id = \"BSC-LT/salamandra-7b-instruct-aina-hack\"\n",
    "\n",
    "# Open the .data file and read the content\n",
    "file_path = './data/output_.data'\n",
    "\n",
    "# Open and read the file content\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    file_content = file.read()\n",
    "\n",
    "# General prompt to summarize the content\n",
    "text = f\"\"\"\n",
    "Please summarize the key points of the following document. Focus on the main topics, headings, and important information. Exclude unnecessary details like references, footnotes, and additional metadata. If the document contains multiple sections, provide a brief summary of each section.\n",
    "\n",
    "Document:\n",
    "{file_content}\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    "  )\n",
    "\n",
    "message = [ { \"role\": \"user\", \"content\": text } ]\n",
    "date_string = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    message,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    date_string=date_string\n",
    ")\n",
    "\n",
    "inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=200)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
